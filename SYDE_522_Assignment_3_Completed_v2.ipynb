{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgv4f9Mn5cet"
      },
      "source": [
        "# Assignment 3 (BONUS): Neural Networks and Backpropagation\n",
        "\n",
        "This bonus assignment has **100 marks (2% Bonus Mark on top of your final grades)**.\n",
        " \n",
        "\n",
        "**Deadline: April 4 at 11:59PM**\n",
        "\n",
        "\n",
        "We will cover a neural networks and backpropgation. It has only 1 problem.\n",
        "\n",
        "Please note that similar codes between individuals will considered violation under the academic integrity policy, and the instructor holds the right to take any necessarry action.\n",
        "\n",
        "## Note\n",
        "* Use Google Colab to do this assignment. \n",
        "* Don't change existing code or text. You can modify some part of the given code by copying them to another array but the main thing is to answer the question with the exact given we give.\n",
        "* Add code and write text as instructed.\n",
        "* Please pay attention to questions asked after writing code to analyze or conclude results (I usually put them in bullet points and bold them)\n",
        "\n",
        "## Submission\n",
        "* File > Download > (as .ipynb)\n",
        "* Submit .ipynb file on the Learn.\n",
        "* Please submit by April 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEs2OwXWzq7u"
      },
      "source": [
        "\n",
        "# Question 1 - Neural Networks (100 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl7lxFoH3H84"
      },
      "source": [
        "In this question, we will build a neural network using only NumPy. It will have a ReLU hidden layer and a sigmoid output layer used for classification. We will implement the backward and forward pass as discussed during the lecture.\n",
        "\n",
        "## Part 1: Training and Testing dataset\n",
        "The most important aspect of any neural network is training and testing data. We are given 8x8 images of handwritten digits from 0-9, and we will predict the digit. First, we will normalize the images. We will solve it as classification problem.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sQ1fUoMrzTCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab7a6f1-b2df-4326-b1ff-0347954ca80d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(320, 64)\n",
            "[[0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]\n",
            " [0]\n",
            " [9]\n",
            " [5]\n",
            " [5]\n",
            " [6]\n",
            " [5]\n",
            " [0]\n",
            " [9]\n",
            " [8]\n",
            " [9]\n",
            " [8]\n",
            " [4]\n",
            " [1]\n",
            " [7]\n",
            " [7]\n",
            " [3]\n",
            " [5]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [2]\n",
            " [2]\n",
            " [7]\n",
            " [8]\n",
            " [2]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [6]\n",
            " [3]\n",
            " [3]\n",
            " [7]\n",
            " [3]\n",
            " [3]\n",
            " [4]\n",
            " [6]\n",
            " [6]\n",
            " [6]\n",
            " [4]\n",
            " [9]\n",
            " [1]\n",
            " [5]\n",
            " [0]\n",
            " [9]\n",
            " [5]\n",
            " [2]\n",
            " [8]\n",
            " [2]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [7]\n",
            " [6]\n",
            " [3]\n",
            " [2]\n",
            " [1]\n",
            " [7]\n",
            " [4]\n",
            " [6]\n",
            " [3]\n",
            " [1]\n",
            " [3]\n",
            " [9]\n",
            " [1]\n",
            " [7]\n",
            " [6]\n",
            " [8]\n",
            " [4]\n",
            " [3]\n",
            " [1]\n",
            " [4]\n",
            " [0]\n",
            " [5]\n",
            " [3]\n",
            " [6]\n",
            " [9]\n",
            " [6]\n",
            " [1]\n",
            " [7]\n",
            " [5]\n",
            " [4]\n",
            " [4]\n",
            " [7]\n",
            " [2]\n",
            " [8]\n",
            " [2]\n",
            " [2]\n",
            " [5]\n",
            " [7]\n",
            " [9]\n",
            " [5]\n",
            " [4]\n",
            " [8]\n",
            " [8]\n",
            " [4]\n",
            " [9]\n",
            " [0]\n",
            " [8]\n",
            " [9]\n",
            " [8]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]\n",
            " [0]\n",
            " [9]\n",
            " [5]\n",
            " [5]\n",
            " [6]\n",
            " [5]\n",
            " [0]\n",
            " [9]\n",
            " [8]\n",
            " [9]\n",
            " [8]\n",
            " [4]\n",
            " [1]\n",
            " [7]\n",
            " [7]\n",
            " [3]\n",
            " [5]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [2]\n",
            " [2]\n",
            " [7]\n",
            " [8]\n",
            " [2]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [6]\n",
            " [3]\n",
            " [3]\n",
            " [7]\n",
            " [3]\n",
            " [3]\n",
            " [4]\n",
            " [6]\n",
            " [6]\n",
            " [6]\n",
            " [4]\n",
            " [9]\n",
            " [1]\n",
            " [5]\n",
            " [0]\n",
            " [9]\n",
            " [5]\n",
            " [2]\n",
            " [8]\n",
            " [2]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [7]\n",
            " [6]\n",
            " [3]\n",
            " [2]\n",
            " [1]\n",
            " [7]\n",
            " [3]\n",
            " [1]\n",
            " [3]\n",
            " [9]\n",
            " [1]\n",
            " [7]\n",
            " [6]\n",
            " [8]\n",
            " [4]\n",
            " [3]\n",
            " [1]\n",
            " [4]\n",
            " [0]\n",
            " [5]\n",
            " [3]\n",
            " [6]\n",
            " [9]\n",
            " [6]\n",
            " [1]\n",
            " [7]\n",
            " [5]\n",
            " [4]\n",
            " [4]\n",
            " [7]\n",
            " [2]\n",
            " [8]\n",
            " [2]\n",
            " [2]\n",
            " [5]\n",
            " [5]\n",
            " [4]\n",
            " [8]\n",
            " [8]\n",
            " [4]\n",
            " [9]\n",
            " [0]\n",
            " [8]\n",
            " [9]\n",
            " [8]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]\n",
            " [0]\n",
            " [9]\n",
            " [5]\n",
            " [5]\n",
            " [6]\n",
            " [5]\n",
            " [0]\n",
            " [9]\n",
            " [8]\n",
            " [9]\n",
            " [8]\n",
            " [4]\n",
            " [1]\n",
            " [7]\n",
            " [7]\n",
            " [3]\n",
            " [5]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [2]\n",
            " [2]\n",
            " [7]\n",
            " [8]\n",
            " [2]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [6]\n",
            " [3]\n",
            " [3]\n",
            " [7]\n",
            " [3]\n",
            " [3]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# DON'T CHANGE THE NORMALIZATION\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "train_x = digits.data[:320]/255.\n",
        "train_y = np.expand_dims(digits.target[:320], axis=1)\n",
        "print(train_x.shape)\n",
        "print(train_y)\n",
        "\n",
        "test_x = digits.data[320:352]/255.\n",
        "test_y = np.expand_dims(digits.target[320:352], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuS2Yv0c0rQT"
      },
      "source": [
        "## Part 2: Building a Neural Network\n",
        "- We will build a 2 hidden-layered neural network (number of neurons per layer is a hyper parameter)\n",
        "- The hidden layer uses ReLU activation. This is the modification from the network presented in Lecture 10\n",
        "- The output layer will be a single neuron sigmoid for the classifier.\n",
        "- The loss function used for training will be the squared loss function.\n",
        "- We remove the bias from each neuron for simplicity\n",
        "\n",
        "**Step 1:** We will define the list of important functions. I give you some of them which I implemented in the lecture and you will need to implement (**8 points**):\n",
        "\n",
        "\n",
        "\n",
        "1.   ReLU\n",
        "2.   Derivative of ReLU (its backward pass)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i-OJz1EvyKBz"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(Y):\n",
        "    n_col = np.amax(Y) + 1\n",
        "    binarized = np.zeros((len(Y), n_col))\n",
        "    for i in range(len(Y)):\n",
        "        binarized[i, Y[i]] = 1.\n",
        "    return binarized\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        " \n",
        "def accuracy(y_pred, y_true):\n",
        "    acc = y_pred.argmax(axis=1) == y_true.argmax(axis=1)\n",
        "    return acc.mean()\n",
        "\n",
        "# ADD the ReLU\n",
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# ADD the derivatives sigmoid and ReLU as defined in the lecture and in the lecture code.\n",
        "def sigmoid_derivative(x): # you can add more input parameters if you want\n",
        "    return np.multiply(x, (1 - x))\n",
        "\n",
        "def ReLU_derivative(x): # you can add more input parameters if you want\n",
        "    return 1 * (x > 0)\n",
        "\n",
        "# Two ways to write the derivative functions\n",
        "\n",
        "# Option - 1 : pass only x as the input parameter\n",
        "# return derivative of x and do the multiplication with the backward gradients of that layer inside the backprop function\n",
        "\n",
        "# Option - 2 : pass both x and backwards_grad as the input parameters\n",
        "# return the product of backwards_grad and derivative of x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p_5Sydf0HFl"
      },
      "source": [
        "**STEP 2:** Lets first define the 2-layers NN. **(15 marks)** \n",
        "- Add initialization yourself \n",
        "- We have added a zero initialization (which in practice is really bad)\n",
        "- Try random uniform between [-1/n_in, 1/n_in] or as I did in the lecture\n",
        "- Define the forwardpass and build the 2-layer NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b2V6KPS70Lna"
      },
      "outputs": [],
      "source": [
        "step_size = 0.1\n",
        "reg = 1e-3 # regularization strength\n",
        "epochs = 100000\n",
        "N = train_y.size\n",
        " \n",
        "# Input features\n",
        "input_size = 64\n",
        " \n",
        "# Hidden layer neurons\n",
        "hidden_size = 8\n",
        " \n",
        "# Output layer\n",
        "output_size = 1\n",
        " \n",
        "results = pd.DataFrame(columns=[\"mse\", \"accuracy\"])\n",
        "\n",
        "np.random.seed(42)\n",
        "# Hidden layer\n",
        "W1 = np.zeros((input_size, hidden_size))\n",
        "W1 = 2.0 * np.random.random((input_size, hidden_size)) - 1\n",
        "# # Output layer\n",
        "W2 = np.zeros((hidden_size , output_size))\n",
        "W2 = 2.0 * np.random.random((hidden_size , output_size)) - 1\n",
        "\n",
        "# You can pass the input and weights for each layer if you want as you defined them above. You can modify this function as you want.\n",
        "def forward_propagation(train_x, W1, W2):\n",
        "    layer1=ReLU(np.dot(train_x, W1))\n",
        "    #print(\"layer1 \", layer1.shape)\n",
        "    output=sigmoid(np.dot(layer1, W2))\n",
        "    #print(\"output \", output.shape)\n",
        "    return layer1, output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK69GhqOiBDp"
      },
      "source": [
        "**STEP 3:** Calculate the loss (**2 points**):\n",
        "\n",
        "- Loss is defined as the squared loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L4GWFf7IiXjG"
      },
      "outputs": [],
      "source": [
        "def loss_f(y_true, y_pred):\n",
        "    error = ((y_pred - y_true)**2).sum()/(y_pred.size)\n",
        "    return error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wUx5NOAmQqM"
      },
      "source": [
        "**STEP 4: Draw the Computational Graph** **(25 points)**\n",
        "\n",
        "Summary of Backward Pass: each layer computes two gradients $\\frac{\\partial y}{\\partial x}$ and $\\frac{\\partial y}{\\partial w}$, where $y$ is output of the layer, $x$ is the input of the layer, and $w$ are parameters of the layers. The layer passes the $\\frac{\\partial y}{\\partial x}$  on to the previous layer and uses $\\frac{\\partial y}{\\partial w}$ to updates its own parameters. We saw that we can either solve it analytically or using computational graphs. **In this assignment you will use computational graphs.**\n",
        "\n",
        "**Note:** We have already coded the backward pass for a similar network with a sigmoid activation function instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVWQ6Dtymwur"
      },
      "source": [
        "**Draw the computational graph for this network**. You can hand sketch it, or use any software to draw it. **Upload a scan image of your solution here.** DO NOT SUBMIT IT in a different document. Show all necessary steps for partial marks including the math. The name of the variables used in the graph should be the same as you use in your code.\n",
        "\n",
        "**UPLOAD SOLUTION HERE:**\n",
        "\n",
        "![graph.png](attachment:graph.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mIwjyE-mG3m"
      },
      "source": [
        "**STEP 5:** Calculate the backward pass (**25 points**)\n",
        "- Revise the lecture\n",
        "- Parameter of each layer updated as: gradients (from the connected layer) * gradients of the layer operations. Use the computational graph from the previous step\n",
        "- Fill the `backward_propagation` function\n",
        "- Calculate the gradient of the error function w.r.t to the `forward_propagation` score. We did that in class now you have to do it using ReLU hidden layer. \n",
        "- **Apply the gradient descent as the gradient update step**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VePD6cGroffx"
      },
      "outputs": [],
      "source": [
        "def back_propagation(output, layer1, y_train, W1, W2):\n",
        "    ## Backpropagate into parameter W2\n",
        "    E1 = 2.0 * (output - y_train)\n",
        "    #print(\"E1 \", E1.shape)\n",
        "    #s0 = np.multiply(output, (1 - output))\n",
        "    s0 = sigmoid_derivative(output)\n",
        "    #print(\"S0 \", s0.shape)\n",
        "    s1 = np.multiply(s0, E1)\n",
        "    #print(\"S1 \", s1.shape)\n",
        "    dW2 = np.dot(layer1.T, s1)\n",
        "    #print(\"dW2 \", dW2.shape)\n",
        "    ## Backpropagate into parameter W1\n",
        "    #s2 = max(0, layer1)\n",
        "    s2 = ReLU_derivative(layer1)\n",
        "    #print(\"s2 \", s2.shape)\n",
        "    s3 = np.dot(s1, W2.T)\n",
        "    #print(\"s3 \", s3.shape)\n",
        "    s4 = np.multiply(s3, s2)\n",
        "    #print(\"s4 \", s4.shape)\n",
        "    dW1 = np.dot(train_x.T, s4)\n",
        "    #print(\"dW1 \", dW1.shape)\n",
        "    return dW1, dW2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T-aqFgZpdO2"
      },
      "source": [
        "**STEP 6: (5 points)** Train the network\n",
        "- Change the hyperparameters LR, reg, epochs, etc....\n",
        "- Build the training model using the functions in all the previous steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K1sY8VzVprnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b772aaa-082f-4e6b-f66f-5567d770a40a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0: loss 23.885018\n",
            "iteration 10000: loss 20.230222\n",
            "iteration 20000: loss 20.213251\n",
            "iteration 30000: loss 20.211003\n",
            "iteration 40000: loss 20.210701\n",
            "iteration 50000: loss 20.210661\n",
            "iteration 60000: loss 20.210655\n",
            "iteration 70000: loss 20.210655\n",
            "iteration 80000: loss 20.210655\n",
            "iteration 90000: loss 20.210655\n"
          ]
        }
      ],
      "source": [
        "#----------------------Training Starts-----------------------------#\n",
        "for i in range(epochs):\n",
        "    ## Forward pass\n",
        "    layer1, output = forward_propagation(train_x, W1, W2)\n",
        "    ## Compute the squared loss\n",
        "    #train_y = to_one_hot(train_y)\n",
        "    data_loss = loss_f(train_y, output)\n",
        "    acc = accuracy(output, train_y)\n",
        "    \n",
        "    reg_loss = 0.5 * reg * np.sum(W1 * W1) + 0.5 * reg * np.sum(W2 * W2)\n",
        "    loss = data_loss + reg_loss\n",
        "#     print(\"data_loss \", data_loss)\n",
        "#     print(\"reg_loss \", reg_loss)\n",
        "#     print(\"i \", i)\n",
        "#     print(\"loss \", loss)\n",
        "    if i % 10000 == 0:\n",
        "        print(\"iteration %d: loss %f\" % (i, loss))\n",
        "    ## Backward pass\n",
        "    #print(\"W1 \", W1.shape)\n",
        "    #print(\"W2 \", W2.shape)\n",
        "    #print(\"train_y \", train_y.shape)\n",
        "    #print(\"output (scores) \", output.shape)\n",
        "    dW1, dW2 = back_propagation(output, layer1, train_y, W1, W2)\n",
        "    ## Add regularization\n",
        "    dW2 += reg * W2\n",
        "    dW1 += reg * W1\n",
        "    \n",
        "    ## Perform parameter update\n",
        "    W1 += -step_size * dW1\n",
        "    W2 += -step_size * dW2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfOJmO-yrBD1"
      },
      "source": [
        "**STEP 7: (5 points)** Test the network\n",
        "- Fill the prediction function to take an test input and apply the trained model on it\n",
        "- Test on all testing data\n",
        "- Report the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HyMMGqWxrf4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47863aa4-73f8-4e5e-f941-70a6942650ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data:  1.0\n"
          ]
        }
      ],
      "source": [
        "def predict(x, W1, W2): # you can add more input parameters if you want\n",
        "    hidden_layer = ReLU(np.dot(x, W1))\n",
        "    scores = sigmoid(np.dot(hidden_layer, W2))\n",
        "    return scores\n",
        "\n",
        "# def accuracy(x): # you can add more input parameters if you want\n",
        "#     accuracy()\n",
        "predicted_scores = predict(test_x, W1, W2)\n",
        "acc = accuracy(predicted_scores, test_y)\n",
        "print(\"Accuracy on test data: \", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3IUZ0W3D8l3"
      },
      "source": [
        "## Part 3 - Tensorflow/Keras Implementation (15 points)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyI5RdrSr8On"
      },
      "source": [
        "Using tensorflow or Keras (like we did in tutorial 9). For this network do the following experiments:\n",
        "\n",
        "\n",
        "\n",
        "*   Start with Implement the same exact network we did in part 1 i.e. hidden layer with ReLU and one output layer using sigmoid for classification. Use SGD as optimizer and the squared loss.\n",
        "*   Change the ReLU to sigmoid and report the accuracy\n",
        "*   Try Adam optimizer and report the accuracy\n",
        "*   Change the loss function to softmax and report the accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FRoETuQ5sUwC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "9ccba91a-5b9a-4e79-f5a7-7449da39ce98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 8)                 520       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 529\n",
            "Trainable params: 529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a51831e8bd05>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Train Neural Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0my_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Building the Neural Network\n",
        "network = models.Sequential()\n",
        "# create additional hidden layers\n",
        "network.add(layers.Dense(8, input_dim=64, activation = 'relu'))\n",
        "# create output layer\n",
        "network.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "# Compile Neural Network\n",
        "network.compile(optimizer='SGD',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(network.summary())\n",
        "# Train Neural Network\n",
        "y_one_hot = to_categorical(train_y, num_classes=1)\n",
        "network.fit(train_x, y_one_hot, epochs=500, batch_size=32, validation_split=0.25, verbose=1)\n",
        "\n",
        "# Test Neural Network\n",
        "loss, accuracy = network.evaluate(train_x, y_one_hot)\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HofGb3VMIaAf"
      },
      "source": [
        "Tabulate the results for the 4 experiments and add to it the results from step 7 in part 1. To generate tables you can use the code I am providing you below and modify the way you want to present it and add as many columns and rows you need. Or you can also use this nice website to create it and save it as image to put it here: https://www.tablesgenerator.com/latex_tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVSguCwwEOgu"
      },
      "source": [
        "\\begin{array}{ccc} \\hline\n",
        "Experiment Type & Validation Accuracy & Testing Accuracy \\\\ \\hline\n",
        "SGD Network & 0.75 & 0.8 \\\\\n",
        "\\end{array}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBN9mbmUeDjV"
      },
      "source": [
        "Discuss the results and comment what was the best loss function and optimizer to use. (2-3 sentences)\n",
        "\n",
        "**write your answer here**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVlebEvfwz-s"
      },
      "source": [
        "Compare the results from this part to part 1 and make your conclusions. (2-3 sentences)\n",
        "\n",
        "**write your answer here**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}